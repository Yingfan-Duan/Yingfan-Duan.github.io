---
layout:     post
title:      "Machine Learning: 知识点汇总"
subtitle:   "面试各类问题知识点整理"
date:       2022-5-4 17:00:00
author:     "Yingfan"
catalog: false
header-style: text
mathjax: true
tags:
  - machine learning
  - study notes
  - interview
---

# 1 基本概念

## 1.1 理解局部最优和全局最优

优化问题一般分为局部最优和全局最优。

（1）局部最优，就是在函数值空间的一个**有限区域**内寻找最小值；而全局最优，是在函数值空间**整个区域**寻找最小值问题

（2）函数局部最小点是它的函数值小于或等于**附近点**的点，但是有可能大于较远距离的点

（3）全局最小点是那种它的函数值小于或等于**所有的可行点**

## 1.2 各种常见算法

日常使用机器学习的任务中，我们经常会遇见各种算法：

- 回归算法
- 聚类算法
- 正则化方法
- 决策树学习
- 贝叶斯方法
- 基于核的算法
- 降维算法
- 集成算法
- 关联规则学习
- 人工神经网络，深度学习

# 2 机器学习学习方式

根据数据类型的不同，对一个问题的建模有不同的方式。依据不同的学习方式和输入数据，机器学习主要分为以下四种学习方式

## 2.1 监督学习

- 定义
  - 监督学习是使用已知正确答案的示例来训练模型。已知数据和其一一对应的标签，训练一个预测模型，将输入数据映射到标签的过程
- 应用场景
  - 分类问题
  - 回归问题
- 算法举例
  - SVM, NB, LR, KNN, DT, RF, AdaBoost, LDA
  - 深度学习(Deep Learning)也是大多数以监督学习的方式呈现

## 2.2 非监督式学习

- 定义
  - 在非监督式学习中，数据并不被特别标识，适用于具有数据集但无标签的情况。训练模型是为了推断出数据的一些内在结构。
- 应用场景
  - 关联规则学习，聚类
- 算法举例
  - Apriori, K-Means, DBSCAN

## 2.3 半监督式学习

- 定义
  - 在此学习方式下，输入数据部分被标记，部分没有被标记，这种学习模型可以用来进行预测。
- 应用场景
  - 应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，通过对已标记数据建模，在此基础上，对未标记数据进行预测
- 算法举例
  - 图论推理算法（Graph Inference）
  - 拉普拉斯支持向量机（Laplacian SVM）

## 2.4 弱监督学习

- 定义
  - 弱监督学习可以看做是有多个标记的数据集合，次集合可以是空集，单个元素，或包含多种情况（没有标记，有一个标记，和有多个标记）的多个元素。 
  - 数据集的标签是不可靠的，这里的不可靠可以是标记不正确，多种标记，标记不充分，局部标记等。已知数据和其一一对应的弱标签，训练一个智能算法，将输入数据映射到一组更强的标签的过程。标签的强弱指的是标签蕴含的信息量的多少，比如相对于分割的标签来说，分类的标签就是弱标签。
- 算法举例
  - 给出一张包含气球的图片，需要得出气球在图片中的位置及气球和背景的分割线， 这就是已知弱标签学习强标签的问题。

# 3 分类算法

## 3.1 常用分类算法的优缺点

| 算法            | 优点 | 缺点 |
| --------------- | ---- | ---- |
| Bayes贝叶斯分类 |      |      |
| Decision Tree   |      |      |
| SVM             |      |      |
| KNN             |      |      |
| NN              |      |      |
| AdaBoost        |      |      |
|                 |      |      |

## 3.2 分类算法的评估方法

分类评估方法主要功能是用来评估分类算法的好坏，而评估一个分类器算法的好坏又包括许多项指 标。了解各种评估方法，在实际应用中选择正确的评估方法是十分重要的。

- **混淆矩阵**

  ![](/img/in-post/post-confusion-matrix.png)

- **评价指标**

  1. **正确率（accuracy）**  

     $accuracy = (TP+TN)/(P+N)$，正确率是被分对的样本数在所有样本数中的占比，通常来说，正确率越高，分类器越好

  2. **错误率（error rate)**  

     描述被分类器错分的比例，对某一个实例来说，分对与分错是互斥事件，所以accuracy = 1 - error rate

  3. **召回率（recall）/ 灵敏度（sensitivity）**

     sensitivity = TP/P，表示的是所有正例中被分对的比例，衡量了分类器对正例的识别能力

  4. **特异性（specificity)**  

     specificity = TN/N，表示的是所有负例中被分对的比例，衡量了分类器对负例的识别能力。

  5. **精度（precision）**  

     precision=TP/(TP+FP)，精度是精确性的度量，表示被分为正例的示例中实际为正例的比例。

  6. **F1-score**

     - $F1=\frac{2precision\times recall}{precision+recall}$
     - 为了综合**多个类别**的分类情况，经常采用的还有微平均F1（micro-averaging）和宏平均F1（macro-averaging ）两种指标
       - 宏平均F1与微平均F1是以两种不同的平均方式求的全局F1指标。
       - 宏平均F1的计算方法先对每个类别单独计算F1值，再取这些F1值的算术平均值作为全局指标。
       - 微平均F1的计算方法是先累加计算各个类别的a、b、c、d的值，再由这些值求出F1值。
       - 宏平均F1平等对待每一个类别，所以它的值主要受到**稀有类别**的影响
       - 而微平均F1平等考虑文档集中的每一个文档，所以它的值受到常见类别 的影响比较大。

  7. **其他评价指标**

     - 计算速度：分类器训练和预测需要的时间； 

     - 鲁棒性：处理缺失值和异常值的能力； 

     - 可扩展性：处理大数据集的能力；

     - 可解释性：分类器的预测标准的可理解性，像决策树产生的规则就是很容易理解的，而神经网络的一堆参数就不好理解，我们只好把它看成一个黑盒子

- **ROC曲线和PR曲线**

  - ROC曲线
    - 灵敏度（TPR）为纵坐标，以1减去特异性（FPR）为横坐标
    - ROC曲线越靠近左 上角，说明其对应模型越可靠
    - 当两个模型的ROC很接近，可以使用AUC来判断模型优劣
  - PR曲线
    - 描述的是precision和recall之间的关系，以recall为横坐 标，precision为纵坐标绘制的曲线
    - 该曲线的所对应的面积AUC实际上是目标检测中常用的评价指标**平均精度（Average Precision, AP）**。AP越高，说明模型性能越好

# 3 逻辑回归

## 3.1 什么是逻辑回归

逻辑回归，假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。

- 逻辑回归的**基本假设**是

  - 数据服从伯努利分布
  - 假设样本为正的概率是sigmoid function

- **损失函数**是

  - 极大似然函数 $L(\beta, x)=\prod_{i=1}^mh(x^i;\beta)^{y^i}\cdot (1-h(x^i;\beta))^{1-y^i}$
  - 其中$h(x^i;\beta)$是sigmoid function

  > 不使用最小二乘法是因为残差不符合正态分布，并且方差不同

- **求解方法**使用梯度下降

  - **批量梯度下降法BGD**
    - 会获得全局最优解
    - 更新每个参数的时候需要遍历所有的数据，计算量会很大
  - **随机梯度下降法SGD**
    - 每次迭代随机选择一个样本计算梯度
    - 可以处理更大量的数据
    - 有可能无法找到最优解
  - **小批量梯度下降法MBGD**
    - 结合了sgd和batch gd的优点，每次更新的时候使用n个样本
    - 减少了参数更新的次数，可以达到更加稳定收敛结果

## 3.2 逻辑回归适用性

逻辑回归可用于以下几个方面：

（1）**用于概率预测**。用于可能性预测时，得到的结果有可比性。比如根据模型进而预测在不同的自变量 情况下，发生某病或某种情况的概率有多大。

（2）**用于分类**。实际上跟预测有些类似，也是根据模型，判断某人属于某病或属于某种情况的概率有多 大，也就是看一下这个人有多大的可能性是属于某病。进行分类时，仅需要设定一个阈值即可，可能性 高于阈值是一类，低于阈值是另一类。

（3）**仅能用于线性问题**。只有当目标和特征是线性关系时，才能用逻辑回归。

## 3.3 逻辑回归的优缺点

- **优点**
  - **可解释性**：形式简单，模型的可解释性非常好
  - 模型效果不错。在工程上是可以接受的（作为baseline)
  - **速度**：训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。
  - 方便调整输出结果，设置不同的阈值进行分类
- **缺点**
  - 只能进行划分线性问题
  - 很难处理数据不平衡的问题
  - 逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后进行逻辑回归

## 3.4 逻辑回归与朴素贝叶斯的区别

逻辑回归与朴素贝叶斯区别有以下几个方面： 

（1）逻辑回归是判别模型， 朴素贝叶斯是生成模型

（2）朴素贝叶斯属于贝叶斯，逻辑回归是最大似然，分属于两种概率哲学

（3）朴素贝叶斯需要条件独立假设。 

（4）逻辑回归需要求特征参数间是线性的

# 4 决策树

## 4.1 决策树的基本原理

- 决策树（Decision Tree）是一种分而治之的决策过程
- 它将一个困难的预测问题，通过树的分支节点划分为两个或多个较为简单的子问题
- 将依规则分割数据集的过程不断递归下去，随着树的深度不断增加，分支节点的子集越来越小，所需要提的问题也逐渐简化
- 当分支节点的深度或者问题的简单程度满足一定的停止规则，该分支节点会停止分裂

## 4.2 决策树的三要素是什么

一棵决策树的生成过程主要分为下3个部分：

1. **特征选择**：从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着 很多不同量化评估标准，从而衍生出不同的决策树算法。
2. **决策树生成**：根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则决策树停止生长。
3. **剪枝**：决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种。

## 4.3 **选择最优划分属性**的办法

一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”（purity）越来越高。下面列出了几种衡量纯度的指标。

- **信息增益（information gain）**
  - 特征A对训练数据集D的信息增益定义为D的信息熵与A条件下的条件熵之差
  - 当D只含有一种类型时信息熵为0
  - **ID3算法**使用此指标作为选择特征的标准
  - 信息增益准则偏向于选择**取值较多的特征**
- **信息增益比（gain ratio）**
  - 特征A对训练数据集D的信息增益率定义为其信息增益与<u>数据集D关于特征A的取值</u>的熵之比
  - 增益率准则对**取值较少的属性**有所偏好
  - **C4.5算法**使用信息增益比，首先从候选划分属性中找出<u>信息增益高于平均水平</u>的属性，再从中<u>选择增益率最高</u>的特征进行划分
- **基尼系数（Gini index）**
  - $G_i=1-\sum_k^np_{i,k}^2$是节点$i$的基尼系数, 其中$p_{i,k}$是节点$i$中第$k$类样本的比例
  - 基尼系数越小，则纯度越高
  - 在候选属性集合A中，选择那个使得<u>划分后基尼指数最小</u>的属性作为最优划分属性
  - **CART**做分类问题时使用基尼系数作为划分标准

## 4.4 决策树优缺点

- **优点**
  - 易理解，机理解释起来简单
  - 可以用于小数据集。
  - 不需要特征标准化
  - 对缺失值不敏感
- **缺点**
  - 容易过拟合
  - 不稳定性：对于数据集旋转，小的变动非常敏感（因为决策边界都是正交的）
    - RF可以通过平均多棵树的结果来解决不稳定性
  - 对于各类别样本数量不一致的数据，在决策树当中，信息增益的结果偏向于那些具有更多数值的特征

## 4.5 CART算法的训练过程

1. 首先寻找阈值$t_k$和特征$k$将训练集划分为两个子集，组合$(k,t_k)$需要最小化损失函数$J(k,t_k)=\frac{m_{left}}{m}G_{left}+\frac{m_{right}}{m}G_{right}$
2. 然后迭代这个分裂过程，直到数据集不可分或当分支节点的深度或者问题的简单程度满足一定的停止规则，该分支节点会停止分裂

> 当使用CART解决回归问题时，将$G_{left}$替换为$MSE_{left}$

## 4.6 决策树剪枝

剪枝处理是决策树学习算法用来解决过拟合问题的一种办法。在决策树算法中，为了尽可能正确分类训练样本， 节点划分过程不断重复， 有时候会造成决策树分支过多，因此可以采用剪枝处理来去掉一些分支来降低过拟合的风险。

剪枝的基本策略有预剪枝（pre-pruning）和后剪枝（post-pruning）

- **预剪枝**
  - 在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点。
  - 优点
    - 降低了过拟合的风险。
    - 减少了决策树的训练时间开销和测试时间开销
  - 缺点
    - 有些分支的当前划分虽不能提升泛化性能、甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却有可能导致性能显著提高。因此预剪枝会有欠拟合的风险。
- **后剪枝**
  - 先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。
  - 优点
    - 欠拟合风险很小。
    - 泛化性能往往优于预剪枝决策树。
  - 缺点
    - 后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中的所有**非叶结点**进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多。

# 5 支持向量机

# 6 贝叶斯分类器

# 7 降维和聚类

# 8 模型评估

# 9 线性判别分析

10 损失函数
